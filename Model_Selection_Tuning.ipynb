{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['outcome', 'carat', 'depth', 'table', 'a1', 'a2', 'a3', 'a4', 'a5',\n",
      "       'b1', 'b2', 'b3', 'b4', 'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7',\n",
      "       'b8', 'b9', 'b10', 'color_E', 'color_F', 'color_G', 'color_H',\n",
      "       'color_I', 'color_J', 'cut_Good', 'cut_Ideal', 'cut_Premium',\n",
      "       'cut_Very Good', 'clarity_IF', 'clarity_SI1', 'clarity_SI2',\n",
      "       'clarity_VS1', 'clarity_VS2', 'clarity_VVS1', 'clarity_VVS2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned training data \n",
    "training_data = pd.read_csv(\"cleaned_training_data.csv\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "X = training_data.drop(columns=['outcome'])\n",
    "y = training_data['outcome']\n",
    "\n",
    "print(training_data.columns)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42),\n",
    "    'SVR': SVR()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    results[name] = {'R2': r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         R2\n",
      "Gradient Boosting  0.473559\n",
      "Random Forest      0.456636\n",
      "XGBoost            0.404290\n",
      "Linear Regression  0.316431\n",
      "Ridge              0.316406\n",
      "SVR                0.314418\n",
      "Lasso              0.281225\n",
      "ElasticNet         0.254423\n",
      "Decision Tree     -0.113727\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values(by='R2', ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R2 Score: 0.3191 ± 0.0172\n",
      "Ridge R2 Score: 0.3191 ± 0.0172\n",
      "Lasso R2 Score: 0.2869 ± 0.0111\n",
      "ElasticNet R2 Score: 0.2594 ± 0.0085\n",
      "Decision Tree R2 Score: -0.1102 ± 0.0380\n",
      "Random Forest R2 Score: 0.4480 ± 0.0105\n",
      "Gradient Boosting R2 Score: 0.4714 ± 0.0123\n",
      "XGBoost R2 Score: 0.3860 ± 0.0227\n",
      "SVR R2 Score: 0.3207 ± 0.0160\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, scoring='r2', cv=5)\n",
    "    print(f\"{name} R2 Score: {scores.mean():.4f} ± {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation R2 Score: 0.4724\n",
      "Cross-Validation R2 Scores: [0.45895287 0.48911621 0.47764884 0.4427623  0.45915226]\n",
      "Mean R2 Score from Cross-Validation: 0.4655 ± 0.0162\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the cleaned training data\n",
    "training_data = pd.read_csv(\"cleaned_training_data6.csv\")\n",
    "\n",
    "# Split into features (X) and target (y)\n",
    "X = training_data.drop(columns=['outcome'])\n",
    "y = training_data['outcome']\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the stacked model\n",
    "estimators = [\n",
    "    ('rf', RandomForestRegressor(random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(random_state=42)),\n",
    "    ('xgb', XGBRegressor(random_state=42))\n",
    "]\n",
    "stack = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
    "\n",
    "# Train the stacked model on the training set\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = stack.predict(X_val)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "print(f\"Validation R2 Score: {val_r2:.4f}\")\n",
    "\n",
    "# Perform 5-fold cross-validation on the training set\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(stack, X_train, y_train, scoring='r2', cv=kf)\n",
    "\n",
    "# Print cross-validation results\n",
    "print(f\"Cross-Validation R2 Scores: {cv_scores}\")\n",
    "print(f\"Mean R2 Score from Cross-Validation: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Results \n",
    "Validation R2 Score: 0.4724\n",
    "Cross-Validation R2 Scores: [0.45895287 0.48911621 0.47764884 0.4427623  0.45915226]\n",
    "Mean R2 Score from Cross-Validation: 0.4655 ± 0.0162"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning \n",
    "Gradient Boosting has the highest R2 score 0.47\n",
    "Random Forest is second with R2 score of 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for Gradient Boosting \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "# Initialize the model\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, \n",
    "                           scoring='r2', cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best R2 Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter tuning results\n",
    "50 min 51.3s\n",
    "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
    "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation R^2 Score: 0.46919072692919117\n",
      "Cross-Validation R^2 Scores: [0.45411405 0.46035867 0.47857845 0.45618376 0.48757568]\n",
      "Mean CV R^2 Score: 0.4673621210563992\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Best hyperparameters obtained from the hyperparameter tuning process\n",
    "best_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 0.9\n",
    "}\n",
    "\n",
    "# Initialize the GradientBoostingRegressor with the best hyperparameters\n",
    "gbr = GradientBoostingRegressor(\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    subsample=best_params['subsample'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Apply RFE to select 20 features\n",
    "rfe = RFE(estimator=gbr, n_features_to_select=20)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Transform the training and validation sets\n",
    "X_train_rfe = rfe.transform(X_train)\n",
    "X_val_rfe = rfe.transform(X_val)\n",
    "\n",
    "# Train the model on the transformed training data\n",
    "gbr.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Predict on the transformed validation set\n",
    "y_val_pred = gbr.predict(X_val_rfe)\n",
    "\n",
    "# Calculate the R^2 score\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "print(f\"Validation R^2 Score: {val_r2}\")\n",
    "\n",
    "# Perform 5-fold cross-validation using a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('rfe', RFE(estimator=gbr, n_features_to_select=20)),\n",
    "    ('gbr', GradientBoostingRegressor(\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        min_samples_leaf=best_params['min_samples_leaf'],\n",
    "        min_samples_split=best_params['min_samples_split'],\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        subsample=best_params['subsample'],\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, scoring='r2', cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-Validation R^2 Scores:\", cv_scores)\n",
    "print(\"Mean CV R^2 Score:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results \n",
    "Validation R^2 Score: 0.46919072692919117\n",
    "Cross-Validation R^2 Scores: [0.45411405 0.46035867 0.47857845 0.45618376 0.48757568]\n",
    "Mean CV R^2 Score: 0.4673621210563992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Use KFold for cross-validation\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    'R2': make_scorer(r2_score),\n",
    "    'MSE': make_scorer(mean_squared_error, greater_is_better=False)\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring=scoring,\n",
    "    refit='R2',  # Refit the model using the best R2 score\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=2  # Display progress\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding R2 score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best R2 Score:\", grid_search.best_score_)\n",
    "\n",
    "# Optional: Evaluate on the validation set\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "val_predictions = best_rf_model.predict(X_val)\n",
    "val_r2 = r2_score(y_val, val_predictions)\n",
    "print(f\"Validation R2 Score: {val_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Best Parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Best R2 Score: 0.471383488591291\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, \n",
    "                           scoring='r2', cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best R2 Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
    "Best Parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
    "Best R2 Score: 0.471383488591291"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean R^2 with cross-validation: 0.4727968459320494\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "training_data = pd.read_csv('cleaned_training_data.csv')\n",
    "test_data = pd.read_csv('CW1_test.csv')\n",
    "\n",
    "# Identify numerical features in the cleaned training data(excluding the target variable)\n",
    "numerical_features = training_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features.remove('outcome') \n",
    "\n",
    "# Apply same scaling transformation\n",
    "scaler = StandardScaler()\n",
    "training_data[numerical_features] = scaler.fit_transform(training_data[numerical_features])\n",
    "test_data[numerical_features] = scaler.transform(test_data[numerical_features])\n",
    "\n",
    "training_data[numerical_features] = scaler.fit_transform(training_data[numerical_features])\n",
    "\n",
    "\n",
    "\n",
    "# One hot encoding for test dataset\n",
    "categorical_cols = ['cut', 'color', 'clarity']\n",
    "test_data = pd.get_dummies(test_data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Align columns\n",
    "test_data = test_data.reindex(columns=training_data.columns.drop('outcome'), fill_value=0)\n",
    "\n",
    "# Separate features and target\n",
    "X_train = training_data.drop(columns=['outcome'])\n",
    "y_train = training_data['outcome']\n",
    "# Best hyperparameters obtained from the hyperameter tuning process \n",
    "best_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 0.9\n",
    "}\n",
    "\n",
    "# Initialize the model with the best hyperparameters\n",
    "gbr = GradientBoostingRegressor(\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    subsample=best_params['subsample'],\n",
    "    random_state=42\n",
    ")\n",
    "# Feature Selection (RFE)\n",
    "base_model = gbr\n",
    "rfe = RFE(estimator=base_model, n_features_to_select=20)\n",
    "rfe.fit(X_train, y_train)\n",
    "X_train_rfe = rfe.transform(X_train)\n",
    "test_data_rfe = rfe.transform(test_data)\n",
    "\n",
    "# Polynomial Features\n",
    "poly = PolynomialFeatures(degree=1, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_rfe)\n",
    "test_data_poly = poly.transform(test_data_rfe)\n",
    "\n",
    "# Model Training\n",
    "model = gbr\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(model, X_train_poly, y_train, scoring='r2', cv=5)\n",
    "print(\"Mean R^2 with cross-validation:\", scores.mean())\n",
    "\n",
    "# Generate predictions\n",
    "y_test_pred = model.predict(test_data_poly)\n",
    "\n",
    "# Save predictions\n",
    "submission = pd.DataFrame({'yhat': y_test_pred})\n",
    "submission.to_csv('CW1_submission_K23004648.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
